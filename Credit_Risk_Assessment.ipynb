{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flVdFqudJiDE"
      },
      "source": [
        "# ***Libraries & Tools***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "uQA5Nz1FJNeL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "from itertools import product\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report, RocCurveDisplay, PrecisionRecallDisplay, make_scorer\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from collections import Counter\n",
        "from sklearn.compose import make_column_transformer, make_column_selector\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXQg5ociOCZF"
      },
      "source": [
        "# ***Data Overview and Exploration***\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLYTo9Qtd_m3"
      },
      "source": [
        "Two datasets are provided.  The original dataset, in the form provided by Prof. Hofmann, contains categorical/symbolic attributes and is in the file \"german.data\". In this project, we utilize the original dataset and convert each categorical/symbolic value of a variable to a numerical value using one hot encoding. One hot encoding is the ideal method for converting variable values because the variables exhibit no inherent order among the categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7serr_-IOHNZ"
      },
      "outputs": [],
      "source": [
        "gcd = pd.read_csv('German_Credit_Data.data', delimiter='\\s+', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "MwTn4REFPxwm",
        "outputId": "1e3adc11-8d5d-463b-af43-8dc456d152d7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "gcd"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-476880ec-58fc-492a-857d-98799484f723\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A11</td>\n",
              "      <td>6</td>\n",
              "      <td>A34</td>\n",
              "      <td>A43</td>\n",
              "      <td>1169</td>\n",
              "      <td>A65</td>\n",
              "      <td>A75</td>\n",
              "      <td>4</td>\n",
              "      <td>A93</td>\n",
              "      <td>A101</td>\n",
              "      <td>...</td>\n",
              "      <td>A121</td>\n",
              "      <td>67</td>\n",
              "      <td>A143</td>\n",
              "      <td>A152</td>\n",
              "      <td>2</td>\n",
              "      <td>A173</td>\n",
              "      <td>1</td>\n",
              "      <td>A192</td>\n",
              "      <td>A201</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A12</td>\n",
              "      <td>48</td>\n",
              "      <td>A32</td>\n",
              "      <td>A43</td>\n",
              "      <td>5951</td>\n",
              "      <td>A61</td>\n",
              "      <td>A73</td>\n",
              "      <td>2</td>\n",
              "      <td>A92</td>\n",
              "      <td>A101</td>\n",
              "      <td>...</td>\n",
              "      <td>A121</td>\n",
              "      <td>22</td>\n",
              "      <td>A143</td>\n",
              "      <td>A152</td>\n",
              "      <td>1</td>\n",
              "      <td>A173</td>\n",
              "      <td>1</td>\n",
              "      <td>A191</td>\n",
              "      <td>A201</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A14</td>\n",
              "      <td>12</td>\n",
              "      <td>A34</td>\n",
              "      <td>A46</td>\n",
              "      <td>2096</td>\n",
              "      <td>A61</td>\n",
              "      <td>A74</td>\n",
              "      <td>2</td>\n",
              "      <td>A93</td>\n",
              "      <td>A101</td>\n",
              "      <td>...</td>\n",
              "      <td>A121</td>\n",
              "      <td>49</td>\n",
              "      <td>A143</td>\n",
              "      <td>A152</td>\n",
              "      <td>1</td>\n",
              "      <td>A172</td>\n",
              "      <td>2</td>\n",
              "      <td>A191</td>\n",
              "      <td>A201</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A11</td>\n",
              "      <td>42</td>\n",
              "      <td>A32</td>\n",
              "      <td>A42</td>\n",
              "      <td>7882</td>\n",
              "      <td>A61</td>\n",
              "      <td>A74</td>\n",
              "      <td>2</td>\n",
              "      <td>A93</td>\n",
              "      <td>A103</td>\n",
              "      <td>...</td>\n",
              "      <td>A122</td>\n",
              "      <td>45</td>\n",
              "      <td>A143</td>\n",
              "      <td>A153</td>\n",
              "      <td>1</td>\n",
              "      <td>A173</td>\n",
              "      <td>2</td>\n",
              "      <td>A191</td>\n",
              "      <td>A201</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A11</td>\n",
              "      <td>24</td>\n",
              "      <td>A33</td>\n",
              "      <td>A40</td>\n",
              "      <td>4870</td>\n",
              "      <td>A61</td>\n",
              "      <td>A73</td>\n",
              "      <td>3</td>\n",
              "      <td>A93</td>\n",
              "      <td>A101</td>\n",
              "      <td>...</td>\n",
              "      <td>A124</td>\n",
              "      <td>53</td>\n",
              "      <td>A143</td>\n",
              "      <td>A153</td>\n",
              "      <td>2</td>\n",
              "      <td>A173</td>\n",
              "      <td>2</td>\n",
              "      <td>A191</td>\n",
              "      <td>A201</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-476880ec-58fc-492a-857d-98799484f723')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-476880ec-58fc-492a-857d-98799484f723 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-476880ec-58fc-492a-857d-98799484f723');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1355031f-7d14-46b4-aeaa-95d73ae1e238\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1355031f-7d14-46b4-aeaa-95d73ae1e238')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1355031f-7d14-46b4-aeaa-95d73ae1e238 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    0   1    2    3     4    5    6   7    8     9   ...    11  12    13  \\\n",
              "0  A11   6  A34  A43  1169  A65  A75   4  A93  A101  ...  A121  67  A143   \n",
              "1  A12  48  A32  A43  5951  A61  A73   2  A92  A101  ...  A121  22  A143   \n",
              "2  A14  12  A34  A46  2096  A61  A74   2  A93  A101  ...  A121  49  A143   \n",
              "3  A11  42  A32  A42  7882  A61  A74   2  A93  A103  ...  A122  45  A143   \n",
              "4  A11  24  A33  A40  4870  A61  A73   3  A93  A101  ...  A124  53  A143   \n",
              "\n",
              "     14 15    16 17    18    19 20  \n",
              "0  A152  2  A173  1  A192  A201  1  \n",
              "1  A152  1  A173  1  A191  A201  2  \n",
              "2  A152  1  A172  2  A191  A201  1  \n",
              "3  A153  1  A173  2  A191  A201  1  \n",
              "4  A153  2  A173  2  A191  A201  2  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gcd.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZGYqj2BLMuo"
      },
      "source": [
        "Replace the column number with column names. The column names are specified in the dataset description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Wgo2l2rSTwLd"
      },
      "outputs": [],
      "source": [
        "column_names = [\n",
        "    'Status of existing checking account',\n",
        "    'Duration in month',\n",
        "    'Credit history',\n",
        "    'Purpose',\n",
        "    'Credit amount',\n",
        "    'Savings account/bonds',\n",
        "    'Present employment since',\n",
        "    'Installment rate in percentage of disposable income',\n",
        "    'Personal status and sex',\n",
        "    'Other debtors / guarantors',\n",
        "    'Present residence since',\n",
        "    'Property',\n",
        "    'Age in years',\n",
        "    'Other installment plans',\n",
        "    'Housing',\n",
        "    'Number of existing credits at this bank',\n",
        "    'Job',\n",
        "    'Number of people being liable to provide maintenance for',\n",
        "    'Telephone',\n",
        "    'foreign worker',\n",
        "    'Class'\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "JCRoKA2eWYpc"
      },
      "outputs": [],
      "source": [
        "columns_mapping = {old_name: new_name for old_name, new_name in zip(gcd.columns, column_names)}\n",
        "\n",
        "gcd.rename(columns=columns_mapping, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMWZnmVpPz8r",
        "outputId": "9ed6e2d3-5db7-435e-f66e-3d5ad0fea086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 21 columns):\n",
            " #   Column                                                    Non-Null Count  Dtype \n",
            "---  ------                                                    --------------  ----- \n",
            " 0   Status of existing checking account                       1000 non-null   object\n",
            " 1   Duration in month                                         1000 non-null   int64 \n",
            " 2   Credit history                                            1000 non-null   object\n",
            " 3   Purpose                                                   1000 non-null   object\n",
            " 4   Credit amount                                             1000 non-null   int64 \n",
            " 5   Savings account/bonds                                     1000 non-null   object\n",
            " 6   Present employment since                                  1000 non-null   object\n",
            " 7   Installment rate in percentage of disposable income       1000 non-null   int64 \n",
            " 8   Personal status and sex                                   1000 non-null   object\n",
            " 9   Other debtors / guarantors                                1000 non-null   object\n",
            " 10  Present residence since                                   1000 non-null   int64 \n",
            " 11  Property                                                  1000 non-null   object\n",
            " 12  Age in years                                              1000 non-null   int64 \n",
            " 13  Other installment plans                                   1000 non-null   object\n",
            " 14  Housing                                                   1000 non-null   object\n",
            " 15  Number of existing credits at this bank                   1000 non-null   int64 \n",
            " 16  Job                                                       1000 non-null   object\n",
            " 17  Number of people being liable to provide maintenance for  1000 non-null   int64 \n",
            " 18  Telephone                                                 1000 non-null   object\n",
            " 19  foreign worker                                            1000 non-null   object\n",
            " 20  Class                                                     1000 non-null   int64 \n",
            "dtypes: int64(8), object(13)\n",
            "memory usage: 164.2+ KB\n"
          ]
        }
      ],
      "source": [
        "gcd.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmhgTIhKLe9V"
      },
      "source": [
        "No missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVLCVgkBP_X_",
        "outputId": "047dda34-5b97-46bd-8a07-32cb7216942d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000, 21)"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gcd.dropna().shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFpOjzELLg4o"
      },
      "source": [
        "No duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijEhD2vIWB2z",
        "outputId": "0beecb00-0c6d-4f18-f899-42b57c740fcf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000, 21)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gcd.drop_duplicates().shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBu53zhPLlBg"
      },
      "source": [
        "There is an imbalance between the classes which we will not address in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx2HOsshc-EL",
        "outputId": "5941d6c2-4e46-4d5f-e91a-3c8772a7a665"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    700\n",
              "2    300\n",
              "Name: Class, dtype: int64"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gcd['Class'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgM7LW5aLzB7"
      },
      "source": [
        "Use one hot encoding to convert the existing variable values to numerical values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "grGU0hB-WMJA"
      },
      "outputs": [],
      "source": [
        "object_cols = gcd.select_dtypes(include=['object']).columns\n",
        "\n",
        "gcd = pd.get_dummies(gcd, columns=object_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLbMnjzgayMf",
        "outputId": "40011fa7-c9c2-4c8d-e653-f2fa5d75783c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 62 columns):\n",
            " #   Column                                                    Non-Null Count  Dtype\n",
            "---  ------                                                    --------------  -----\n",
            " 0   Duration in month                                         1000 non-null   int64\n",
            " 1   Credit amount                                             1000 non-null   int64\n",
            " 2   Installment rate in percentage of disposable income       1000 non-null   int64\n",
            " 3   Present residence since                                   1000 non-null   int64\n",
            " 4   Age in years                                              1000 non-null   int64\n",
            " 5   Number of existing credits at this bank                   1000 non-null   int64\n",
            " 6   Number of people being liable to provide maintenance for  1000 non-null   int64\n",
            " 7   Class                                                     1000 non-null   int64\n",
            " 8   Status of existing checking account_A11                   1000 non-null   uint8\n",
            " 9   Status of existing checking account_A12                   1000 non-null   uint8\n",
            " 10  Status of existing checking account_A13                   1000 non-null   uint8\n",
            " 11  Status of existing checking account_A14                   1000 non-null   uint8\n",
            " 12  Credit history_A30                                        1000 non-null   uint8\n",
            " 13  Credit history_A31                                        1000 non-null   uint8\n",
            " 14  Credit history_A32                                        1000 non-null   uint8\n",
            " 15  Credit history_A33                                        1000 non-null   uint8\n",
            " 16  Credit history_A34                                        1000 non-null   uint8\n",
            " 17  Purpose_A40                                               1000 non-null   uint8\n",
            " 18  Purpose_A41                                               1000 non-null   uint8\n",
            " 19  Purpose_A410                                              1000 non-null   uint8\n",
            " 20  Purpose_A42                                               1000 non-null   uint8\n",
            " 21  Purpose_A43                                               1000 non-null   uint8\n",
            " 22  Purpose_A44                                               1000 non-null   uint8\n",
            " 23  Purpose_A45                                               1000 non-null   uint8\n",
            " 24  Purpose_A46                                               1000 non-null   uint8\n",
            " 25  Purpose_A48                                               1000 non-null   uint8\n",
            " 26  Purpose_A49                                               1000 non-null   uint8\n",
            " 27  Savings account/bonds_A61                                 1000 non-null   uint8\n",
            " 28  Savings account/bonds_A62                                 1000 non-null   uint8\n",
            " 29  Savings account/bonds_A63                                 1000 non-null   uint8\n",
            " 30  Savings account/bonds_A64                                 1000 non-null   uint8\n",
            " 31  Savings account/bonds_A65                                 1000 non-null   uint8\n",
            " 32  Present employment since_A71                              1000 non-null   uint8\n",
            " 33  Present employment since_A72                              1000 non-null   uint8\n",
            " 34  Present employment since_A73                              1000 non-null   uint8\n",
            " 35  Present employment since_A74                              1000 non-null   uint8\n",
            " 36  Present employment since_A75                              1000 non-null   uint8\n",
            " 37  Personal status and sex_A91                               1000 non-null   uint8\n",
            " 38  Personal status and sex_A92                               1000 non-null   uint8\n",
            " 39  Personal status and sex_A93                               1000 non-null   uint8\n",
            " 40  Personal status and sex_A94                               1000 non-null   uint8\n",
            " 41  Other debtors / guarantors_A101                           1000 non-null   uint8\n",
            " 42  Other debtors / guarantors_A102                           1000 non-null   uint8\n",
            " 43  Other debtors / guarantors_A103                           1000 non-null   uint8\n",
            " 44  Property_A121                                             1000 non-null   uint8\n",
            " 45  Property_A122                                             1000 non-null   uint8\n",
            " 46  Property_A123                                             1000 non-null   uint8\n",
            " 47  Property_A124                                             1000 non-null   uint8\n",
            " 48  Other installment plans_A141                              1000 non-null   uint8\n",
            " 49  Other installment plans_A142                              1000 non-null   uint8\n",
            " 50  Other installment plans_A143                              1000 non-null   uint8\n",
            " 51  Housing_A151                                              1000 non-null   uint8\n",
            " 52  Housing_A152                                              1000 non-null   uint8\n",
            " 53  Housing_A153                                              1000 non-null   uint8\n",
            " 54  Job_A171                                                  1000 non-null   uint8\n",
            " 55  Job_A172                                                  1000 non-null   uint8\n",
            " 56  Job_A173                                                  1000 non-null   uint8\n",
            " 57  Job_A174                                                  1000 non-null   uint8\n",
            " 58  Telephone_A191                                            1000 non-null   uint8\n",
            " 59  Telephone_A192                                            1000 non-null   uint8\n",
            " 60  foreign worker_A201                                       1000 non-null   uint8\n",
            " 61  foreign worker_A202                                       1000 non-null   uint8\n",
            "dtypes: int64(8), uint8(54)\n",
            "memory usage: 115.4 KB\n"
          ]
        }
      ],
      "source": [
        "gcd.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5dCaM-bchLN"
      },
      "source": [
        "# ***Data Mining and Classification***\n",
        "***Note: Class 1 = Good Customer, Class 2 = Bad Customer***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTT63vbkd0TG"
      },
      "source": [
        "The rows represent the actual classification and The columns the predicted classification.\n",
        "\n",
        "It is worse to class a customer as good when they are bad (5), than it is to class a customer as bad when they are good (1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "3qJYvlyqcrA2"
      },
      "outputs": [],
      "source": [
        "ORIGINAL_COST = [\n",
        "    [0,1],\n",
        "    [5,0]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GVtmjEslhKE"
      },
      "source": [
        "## ***Feature Selection using SVM***\n",
        "Feature selection with a linear SVM is time consuming. However, we can use undersampling to take all the instances of the minority class and reduce the size of the majority class. Use this methodology only if you want to improve the results of the classification. Otherwise, this methodology takes a lot of time to run.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "minority_class = gcd[gcd['Class'] == 2]\n",
        "majority_class = gcd[gcd['Class'] == 1]\n",
        "\n",
        "minority_sample = minority_class\n",
        "\n",
        "majority_sample = majority_class.sample(n=300, random_state=42)\n",
        "\n",
        "gcd_balanced = pd.concat([minority_sample, majority_sample])"
      ],
      "metadata": {
        "id": "Ak8YZA88So75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcd_balanced['Class'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0Dz5YmQXejq",
        "outputId": "804ca590-2b2e-41e6-db05-39bc3844470f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    300\n",
              "1    300\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGGzKOQ4lmU1"
      },
      "outputs": [],
      "source": [
        "def calc_cost(y_true, y_pred, cost_matrix):\n",
        "    conf = confusion_matrix(y_true,y_pred).T\n",
        "    return np.sum(conf * cost_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcXjC8aDmWqg"
      },
      "outputs": [],
      "source": [
        "def find_best_params(estimator, params, train_set, validation_set, starting_point = 0):\n",
        "    train_x = train_set[0]\n",
        "    train_y = train_set[1]\n",
        "    test_x = validation_set[0]\n",
        "    test_y = validation_set[1]\n",
        "\n",
        "    min_cost = np.inf\n",
        "    best_params = {}\n",
        "    full_params_set = []\n",
        "\n",
        "    if type(params) == dict:\n",
        "\n",
        "        for values in product(*params.values()):\n",
        "            point = dict(zip(params.keys(), values))\n",
        "            full_params_set.append(point)\n",
        "\n",
        "    elif type(params) == list:\n",
        "\n",
        "        for params_subset in params:\n",
        "            for values in product(*params_subset.values()):\n",
        "                point = dict(zip(params_subset.keys(), values))\n",
        "                full_params_set.append(point)\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "    steps = len(full_params_set)\n",
        "    counter = starting_point\n",
        "    print(f\"Testing {steps} models in total.\")\n",
        "    start = time.time()\n",
        "\n",
        "    for params in full_params_set[starting_point:]:\n",
        "\n",
        "        estimator.set_params(**params)\n",
        "        estimator.fit(train_x,train_y)\n",
        "        pred_y = estimator.predict(test_x)\n",
        "        cost_matrix = np.matrix([[0,1], [5,0]])\n",
        "        cost = calc_cost(test_y, pred_y, cost_matrix)\n",
        "\n",
        "        if cost < min_cost:\n",
        "            min_cost = cost\n",
        "            best_params = params\n",
        "\n",
        "        print(\"________________________________________________________________________________________\")\n",
        "        print(f\"{counter}/{steps} | Cost: {cost} | Elapsed: {int((time.time()-start)*100)/100}s | {params}\")\n",
        "        print(\"________________________________________________________________________________________\")\n",
        "\n",
        "    return best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvwRtOKlmcO5"
      },
      "outputs": [],
      "source": [
        "linear_params = {\"C\":[0.1,1.0,10.0,100.0],\"kernel\":[\"linear\"]}\n",
        "\n",
        "y = gcd['Class']\n",
        "x = gcd.drop(columns = ['Class'])\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(x, y, test_size = 0.20)\n",
        "x_train,x_val,y_train,y_val = train_test_split(x_train, y_train, test_size = 0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6Y5odF7meJ4"
      },
      "outputs": [],
      "source": [
        "best_params = find_best_params(SVC(), linear_params, [x_train, y_train], [x_val, y_val])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVXDIZbtmgSK"
      },
      "outputs": [],
      "source": [
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJjILaV-mid-"
      },
      "outputs": [],
      "source": [
        "classifier = SVC(*best_params)\n",
        "classifier.fit(x_train, y_train)\n",
        "weights = zip(list(gcd.columns), classifier.coef_.todense().data)\n",
        "weights_sorted = {k: v for k, v in sorted(weights.items(), key=lambda item: item[1])}\n",
        "with open(\"Important_SVC_Features.json\",\"w\") as f:\n",
        "    json.dump(weights_sorted,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEDy3MBmhzrj"
      },
      "source": [
        "## ***Feature Selection using GridSearch with RF***\n",
        "We can use the GridSearch method with the Random Forest algorithm to evaluate the importance of each variable and identify the optimal features for classifying customer types more effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq6Rf8QDh5r_"
      },
      "outputs": [],
      "source": [
        "y = gcd['Class']\n",
        "x = gcd.drop(columns = ['Class'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBS6sXYbiFLk"
      },
      "outputs": [],
      "source": [
        "def custom_score(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    cost = np.sum(cm * ORIGINAL_COST)\n",
        "    return -cost  # Minimize cost, so negative of cost is returned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddMyY7-QiJB8"
      },
      "outputs": [],
      "source": [
        "custom_scorer = make_scorer(custom_score, greater_is_better = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYPXZ808ihwp"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, n_jobs=-1, scoring=custom_scorer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "id": "bDPX5uxtirqF",
        "outputId": "7a4fa3a3-fd16-4d3a-adaa-2f1cd0caeaaf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42), n_jobs=-1,\n",
              "             param_grid={'max_depth': [None, 10, 20],\n",
              "                         'min_samples_split': [2, 5, 10],\n",
              "                         'n_estimators': [50, 100, 200]},\n",
              "             scoring=make_scorer(custom_score, greater_is_better=False))"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42), n_jobs=-1,\n",
              "             param_grid={&#x27;max_depth&#x27;: [None, 10, 20],\n",
              "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
              "                         &#x27;n_estimators&#x27;: [50, 100, 200]},\n",
              "             scoring=make_scorer(custom_score, greater_is_better=False))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42), n_jobs=-1,\n",
              "             param_grid={&#x27;max_depth&#x27;: [None, 10, 20],\n",
              "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
              "                         &#x27;n_estimators&#x27;: [50, 100, 200]},\n",
              "             scoring=make_scorer(custom_score, greater_is_better=False))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "grid_search.fit(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi-aQvIOiuaz",
        "outputId": "742672eb-28fc-47a6-ecbf-b520bb5c55fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100}\n"
          ]
        }
      ],
      "source": [
        "best_params = grid_search.best_params_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "best_rf_classifier = grid_search.best_estimator_\n",
        "\n",
        "best_rf_classifier.fit(x, y)\n",
        "\n",
        "feature_importances = best_rf_classifier.feature_importances_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKUNlIAKNi8-",
        "outputId": "45757412-e1d3-42dc-f814-d7e55d749020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status of existing checking account: 0.18127052680713\n",
            "Credit amount: 0.09963120887039098\n",
            "Duration in month: 0.08502755277969097\n",
            "Credit history: 0.07943903009449499\n",
            "Purpose: 0.07526519681385582\n",
            "Age in years: 0.07131138040402563\n",
            "Savings account/bonds: 0.05696442042004994\n",
            "Present employment since: 0.05243765670601543\n",
            "Property: 0.044531448912716966\n",
            "Other installment plans: 0.03890869221327936\n",
            "Personal status and sex: 0.03212342377332732\n",
            "Job: 0.030707166210147484\n",
            "Housing: 0.030348743444134216\n",
            "Installment rate in percentage of disposable income: 0.026335110898120195\n",
            "Other debtors / guarantors: 0.025730195747058995\n",
            "Present residence since: 0.022868905519949165\n",
            "Telephone: 0.017995911650055568\n",
            "Number of existing credits at this bank: 0.013141854015556249\n",
            "foreign worker: 0.008117241945819281\n",
            "Number of people being liable to provide maintenance for: 0.007844332774181536\n"
          ]
        }
      ],
      "source": [
        "feature_importance_sum = {}\n",
        "\n",
        "for feature, importance in zip(x.columns, feature_importances):\n",
        "    feature_name = re.sub(r'^(.*?)_.+', r'\\1', feature)\n",
        "\n",
        "    if feature_name in feature_importance_sum:\n",
        "        feature_importance_sum[feature_name] += importance\n",
        "    else:\n",
        "        feature_importance_sum[feature_name] = importance\n",
        "\n",
        "sorted_feature_importance_sum = sorted(feature_importance_sum.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for feature, importance_sum in sorted_feature_importance_sum:\n",
        "    print(f\"{feature}: {importance_sum}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1-GPC-6RJZL",
        "outputId": "c898b1a8-9fb1-4543-c669-da8108e1456d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status of existing checking account_A11\n",
            "Status of existing checking account_A12\n",
            "Status of existing checking account_A13\n",
            "Status of existing checking account_A14\n",
            "Credit amount\n",
            "Duration in month\n",
            "Credit history_A30\n",
            "Credit history_A31\n",
            "Credit history_A32\n",
            "Credit history_A33\n",
            "Credit history_A34\n",
            "Purpose_A40\n",
            "Purpose_A41\n",
            "Purpose_A410\n",
            "Purpose_A42\n",
            "Purpose_A43\n",
            "Purpose_A44\n",
            "Purpose_A45\n",
            "Purpose_A46\n",
            "Purpose_A48\n",
            "Purpose_A49\n",
            "Age in years\n",
            "Savings account/bonds_A61\n",
            "Savings account/bonds_A62\n",
            "Savings account/bonds_A63\n",
            "Savings account/bonds_A64\n",
            "Savings account/bonds_A65\n",
            "Present employment since_A71\n",
            "Present employment since_A72\n",
            "Present employment since_A73\n",
            "Present employment since_A74\n",
            "Present employment since_A75\n",
            "Property_A121\n",
            "Property_A122\n",
            "Property_A123\n",
            "Property_A124\n",
            "Other installment plans_A141\n",
            "Other installment plans_A142\n",
            "Other installment plans_A143\n",
            "Personal status and sex_A91\n",
            "Personal status and sex_A92\n",
            "Personal status and sex_A93\n",
            "Personal status and sex_A94\n",
            "Job_A171\n",
            "Job_A172\n",
            "Job_A173\n",
            "Job_A174\n",
            "Housing_A151\n",
            "Housing_A152\n",
            "Housing_A153\n"
          ]
        }
      ],
      "source": [
        "top_features = [sorted_feature_importance_sum[i][0] for i in range(13)]\n",
        "\n",
        "complete_features = []\n",
        "\n",
        "for feature in top_features:\n",
        "  for column in gcd.columns:\n",
        "      if column.startswith(feature):\n",
        "          suffix = column[len(feature):].lstrip('_')\n",
        "          if suffix != '':\n",
        "            complete_part = feature + '_' + suffix\n",
        "          else:\n",
        "            complete_part = feature\n",
        "\n",
        "          complete_features.append(complete_part)\n",
        "\n",
        "top_features = complete_features\n",
        "for complete_feat in top_features: print(complete_feat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiAoBvBJkH05"
      },
      "source": [
        "## ***Classification***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset into a training set (75%) and a testing set (25%).\n",
        "\n"
      ],
      "metadata": {
        "id": "dapAk8iNYiOp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Q0xqE1S9ekV3"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(gcd.drop(columns=['Class']), gcd['Class'], test_size=0.25, random_state=42, stratify=gcd['Class'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_names = [\n",
        "    'Random Forests', 'Linear SVM', 'Naive Bayes'\n",
        "]\n",
        "\n",
        "classifiers = [RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, min_samples_split=10),\n",
        "               SVC(kernel='linear'),\n",
        "               GaussianNB()\n",
        "               ]"
      ],
      "metadata": {
        "id": "rHQDdBChsXq7"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Cost-based Evaluation***\n",
        "We run three classifiers, evaluate their performance, and calculate the total cost of their results."
      ],
      "metadata": {
        "id": "sZnQhx41ZcWz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YITndbffKbF",
        "outputId": "7815ef22-8a12-4fc9-9975-baf284c52a6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.79      0.93      0.85       175\n",
            "           2       0.70      0.41      0.52        75\n",
            "\n",
            "    accuracy                           0.77       250\n",
            "   macro avg       0.75      0.67      0.69       250\n",
            "weighted avg       0.76      0.77      0.75       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[162  13]\n",
            " [ 44  31]]\n",
            "\n",
            "Total Cost: 233\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.79      0.88      0.83       175\n",
            "           2       0.62      0.47      0.53        75\n",
            "\n",
            "    accuracy                           0.76       250\n",
            "   macro avg       0.71      0.67      0.68       250\n",
            "weighted avg       0.74      0.76      0.74       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[154  21]\n",
            " [ 40  35]]\n",
            "\n",
            "Total Cost: 221\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.82      0.75      0.79       175\n",
            "           2       0.52      0.63      0.57        75\n",
            "\n",
            "    accuracy                           0.72       250\n",
            "   macro avg       0.67      0.69      0.68       250\n",
            "weighted avg       0.73      0.72      0.72       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[132  43]\n",
            " [ 28  47]]\n",
            "\n",
            "Total Cost: 183\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for name, clf in zip(classifier_names, classifiers):\n",
        "  print(name)\n",
        "  clf.fit(x_train, y_train)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above results we can conclude the following:\n",
        "- The NB (Naive Bayes) has the lowest cost\n",
        "- The NB classifies 28 customers as \"Good\" even though they are \"Bad\". Compared to the other algorithms, NB has the lowest count for the FP (False Positives) but the highest count for the FN (False Negatives - Customers that are \"Good\" but are classified as \"Bad\")\n",
        "- The RF (Random Forest) algorithm has the lowest count for the FN, which means that it is very good at classifying \"Good\" customers\n",
        "- The performance of the SVM falls somewhere in between the other two algorithms\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PdtQh9LbfFJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Minimizing the Expected Cost***"
      ],
      "metadata": {
        "id": "FHKaF4wZsEQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_expected_cost_classifiers = [\n",
        "    RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, min_samples_split=10),\n",
        "    SVC(kernel='linear', probability=True),\n",
        "    GaussianNB()\n",
        "]"
      ],
      "metadata": {
        "id": "RbeSOeNe8gjk"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: Before we use .predict_proba, we have to replace the category values of the target variable. So 1 (\"Good\") will be 0 and 2 (\"Bad\") will be 1.**"
      ],
      "metadata": {
        "id": "HyNDqvKTC_s0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new dataframe to encode (replace) the target variable values\n",
        "gcd_encoded_classes = gcd.copy()"
      ],
      "metadata": {
        "id": "R-RH4U_KBMPL"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcd_encoded_classes['Class'] = gcd_encoded_classes['Class'].replace({1: 0, 2: 1})"
      ],
      "metadata": {
        "id": "W6xytS4iBWZy"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new training and testing set using the new dataframe\n",
        "x_train, x_test, y_train, y_test = train_test_split(gcd_encoded_classes.drop(columns=['Class']), gcd_encoded_classes['Class'], test_size=0.25, random_state=42, stratify=gcd_encoded_classes['Class'])"
      ],
      "metadata": {
        "id": "FVKpW_sPBq1s"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost minimization without probability calibration**"
      ],
      "metadata": {
        "id": "s_XZ7Nm05X7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, clf in zip(classifier_names, min_expected_cost_classifiers):\n",
        "  print(name)\n",
        "  model = clf.fit(x_train, y_train)\n",
        "  y_pred_prob = model.predict_proba(x_test)\n",
        "  y_pred = np.argmin(np.matmul(y_pred_prob, np.array(ORIGINAL_COST)), axis=1)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ],
      "metadata": {
        "id": "QYdkFw8ysOa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fff10d0-31b0-4d8f-d230-a70775da405f"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.38      0.54       175\n",
            "           1       0.40      0.96      0.56        75\n",
            "\n",
            "    accuracy                           0.55       250\n",
            "   macro avg       0.68      0.67      0.55       250\n",
            "weighted avg       0.79      0.55      0.55       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[ 66 109]\n",
            " [  3  72]]\n",
            "\n",
            "Total Cost: 124\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.46      0.60       175\n",
            "           1       0.39      0.83      0.53        75\n",
            "\n",
            "    accuracy                           0.57       250\n",
            "   macro avg       0.63      0.64      0.57       250\n",
            "weighted avg       0.72      0.57      0.58       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[80 95]\n",
            " [13 62]]\n",
            "\n",
            "Total Cost: 160\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.68      0.76       175\n",
            "           1       0.50      0.75      0.60        75\n",
            "\n",
            "    accuracy                           0.70       250\n",
            "   macro avg       0.68      0.71      0.68       250\n",
            "weighted avg       0.75      0.70      0.71       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[119  56]\n",
            " [ 19  56]]\n",
            "\n",
            "Total Cost: 151\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above results we can conclude that:\n",
        "- The Naive Bayes (NB) and Support Vector Machine (SVM) algorithms are the primary contenders for minimizing cost without the need for probability calibration.\n",
        "- The Naive Bayes (NB) algorithm exhibits a higher count of False Positives compared to SVM. However, it also demonstrates the lowest count of False Negatives and the most significant difference in True Positives. If one of our priorities is to minimize the loss of \"Good\" clients while effectively identifying and managing \"Bad\" clients, then NB emerges as the preferred choice.\n",
        "- The Support Vector Machine (SVM) displays a lower count of False Positives compared to NB, albeit with a significantly higher count of False Negatives. If prioritizing the preservation of \"Good\" clients is not our primary concern, then SVM emerges as the preferred choice."
      ],
      "metadata": {
        "id": "F3UbZuFHzha7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost minimization with sigmoid calibration**"
      ],
      "metadata": {
        "id": "X2AT66P0Ctt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, clf in zip(classifier_names, min_expected_cost_classifiers):\n",
        "  print(name)\n",
        "  cc = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=3)\n",
        "  model = cc.fit(x_train, y_train)\n",
        "  y_pred_prob = model.predict_proba(x_test)\n",
        "  y_pred = np.argmin(np.matmul(y_pred_prob, np.array(ORIGINAL_COST)), axis=1)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqIFZBgtCq4B",
        "outputId": "e4926dd3-d2b3-4761-8ed1-030a5384c823"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.46      0.61       175\n",
            "           1       0.42      0.89      0.57        75\n",
            "\n",
            "    accuracy                           0.59       250\n",
            "   macro avg       0.66      0.68      0.59       250\n",
            "weighted avg       0.76      0.59      0.60       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[81 94]\n",
            " [ 8 67]]\n",
            "\n",
            "Total Cost: 134\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.32      0.47       175\n",
            "           1       0.37      0.93      0.53        75\n",
            "\n",
            "    accuracy                           0.50       250\n",
            "   macro avg       0.64      0.63      0.50       250\n",
            "weighted avg       0.75      0.50      0.49       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[ 56 119]\n",
            " [  5  70]]\n",
            "\n",
            "Total Cost: 144\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.46      0.61       175\n",
            "           1       0.41      0.85      0.55        75\n",
            "\n",
            "    accuracy                           0.58       250\n",
            "   macro avg       0.64      0.66      0.58       250\n",
            "weighted avg       0.74      0.58      0.59       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[81 94]\n",
            " [11 64]]\n",
            "\n",
            "Total Cost: 149\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above results we can conclude that\n",
        "- The Random Forest (RF) emerges as the optimal algorithm in this scenario, as it effectively balances True Positives and False Negatives while simultaneously minimizing the count of False Positives."
      ],
      "metadata": {
        "id": "kUWYLIqMx36Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost Minimization with isotonic calibration**"
      ],
      "metadata": {
        "id": "SXe8ARJYDbro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, clf in zip(classifier_names, min_expected_cost_classifiers):\n",
        "  print(name)\n",
        "  cc = CalibratedClassifierCV(clf, method=\"isotonic\", cv=3)\n",
        "  model = cc.fit(x_train, y_train)\n",
        "  y_pred_prob = model.predict_proba(x_test)\n",
        "  y_pred = np.argmin(np.matmul(y_pred_prob, np.array(ORIGINAL_COST)), axis=1)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_8fNMPIDiNL",
        "outputId": "f038f166-2a37-4f09-9efa-7e1c8f764274"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.50      0.65       175\n",
            "           1       0.43      0.88      0.58        75\n",
            "\n",
            "    accuracy                           0.62       250\n",
            "   macro avg       0.67      0.69      0.61       250\n",
            "weighted avg       0.76      0.62      0.63       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[88 87]\n",
            " [ 9 66]]\n",
            "\n",
            "Total Cost: 132\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.38      0.53       175\n",
            "           1       0.39      0.92      0.55        75\n",
            "\n",
            "    accuracy                           0.54       250\n",
            "   macro avg       0.65      0.65      0.54       250\n",
            "weighted avg       0.76      0.54      0.54       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[ 66 109]\n",
            " [  6  69]]\n",
            "\n",
            "Total Cost: 139\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.50      0.64       175\n",
            "           1       0.42      0.85      0.56        75\n",
            "\n",
            "    accuracy                           0.60       250\n",
            "   macro avg       0.65      0.68      0.60       250\n",
            "weighted avg       0.75      0.60      0.62       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[87 88]\n",
            " [11 64]]\n",
            "\n",
            "Total Cost: 143\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above results we can conclude that:\n",
        "- All algorithms exhibit a low count of False Positives but a high count of False Negatives with the SVM having the highest count and the biggest difference between True Positives and False Negatives. Nevertheless, the Naive Bayes and Random Forest algorithms excel in this regard by effectively balancing True Positives with False Negatives, a balance not achieved by the SVM."
      ],
      "metadata": {
        "id": "dYa0IUfcwskb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Rebalancing***"
      ],
      "metadata": {
        "id": "Vq9VwFVInqbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Undersampling**"
      ],
      "metadata": {
        "id": "XYxM8EG5oinE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VISeQcllrbpt",
        "outputId": "2252810b-c883-43d5-e6ae-66b7a095d4dc"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({1: 525, 2: 225})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We reduce the size of the first class (\"Good\") with undersampling to be equal to the size of the second class (\"Bad\")."
      ],
      "metadata": {
        "id": "STxaC6WGt28b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = RandomUnderSampler(sampling_strategy={1: 225, 2: 225}, random_state=42)\n",
        "x_rs, y_rs = sampler.fit_resample(x_train, y_train)\n",
        "\n",
        "for name, clf in zip(classifier_names, classifiers):\n",
        "  print(name)\n",
        "  clf.fit(x_rs, y_rs)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxrKBnSSn6m-",
        "outputId": "c0bf07c8-dfed-4738-e561-8e2e033774af"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.87      0.68      0.76       175\n",
            "           2       0.50      0.76      0.61        75\n",
            "\n",
            "    accuracy                           0.70       250\n",
            "   macro avg       0.69      0.72      0.68       250\n",
            "weighted avg       0.76      0.70      0.72       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[119  56]\n",
            " [ 18  57]]\n",
            "\n",
            "Total Cost: 146\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.87      0.59      0.71       175\n",
            "           2       0.46      0.80      0.58        75\n",
            "\n",
            "    accuracy                           0.66       250\n",
            "   macro avg       0.67      0.70      0.65       250\n",
            "weighted avg       0.75      0.66      0.67       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[104  71]\n",
            " [ 15  60]]\n",
            "\n",
            "Total Cost: 146\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.83      0.71      0.77       175\n",
            "           2       0.50      0.67      0.57        75\n",
            "\n",
            "    accuracy                           0.70       250\n",
            "   macro avg       0.67      0.69      0.67       250\n",
            "weighted avg       0.73      0.70      0.71       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[125  50]\n",
            " [ 25  50]]\n",
            "\n",
            "Total Cost: 175\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Oversampling**:\n",
        "We increase the size of the minority class (\"Bad\") with oversampling to be equal to the size of the majority class (\"Good\")."
      ],
      "metadata": {
        "id": "cL8yn9aOtwJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = RandomOverSampler(sampling_strategy={1: 525, 2: 525}, random_state=42)\n",
        "x_rs, y_rs = sampler.fit_resample(x_train, y_train)\n",
        "\n",
        "for name, clf in zip(classifier_names, classifiers):\n",
        "  print(name)\n",
        "  clf.fit(x_rs, y_rs)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8YMPUqYtYp9",
        "outputId": "eae17515-43b2-4c73-ea20-b75d104f8f9f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.84      0.76      0.80       175\n",
            "           2       0.54      0.65      0.59        75\n",
            "\n",
            "    accuracy                           0.73       250\n",
            "   macro avg       0.69      0.71      0.69       250\n",
            "weighted avg       0.75      0.73      0.73       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[133  42]\n",
            " [ 26  49]]\n",
            "\n",
            "Total Cost: 172\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.85      0.61      0.71       175\n",
            "           2       0.45      0.76      0.57        75\n",
            "\n",
            "    accuracy                           0.65       250\n",
            "   macro avg       0.65      0.68      0.64       250\n",
            "weighted avg       0.73      0.65      0.67       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[106  69]\n",
            " [ 18  57]]\n",
            "\n",
            "Total Cost: 159\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.85      0.70      0.76       175\n",
            "           2       0.50      0.71      0.59        75\n",
            "\n",
            "    accuracy                           0.70       250\n",
            "   macro avg       0.67      0.70      0.68       250\n",
            "weighted avg       0.74      0.70      0.71       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[122  53]\n",
            " [ 22  53]]\n",
            "\n",
            "Total Cost: 163\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combination of undersampling and oversampling**: We reduce the number of samples in the majority class and increase the number of samples in the minority class."
      ],
      "metadata": {
        "id": "Adzl66sau9d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = RandomUnderSampler(sampling_strategy={1: 225, 2: 225}, random_state=42)\n",
        "x_rs, y_rs = sampler.fit_resample(x_train, y_train)\n",
        "sampler = RandomOverSampler(sampling_strategy={1: 225, 2: 525}, random_state=42)\n",
        "x_rs, y_rs = sampler.fit_resample(x_rs, y_rs)\n",
        "\n",
        "for name, clf in zip(classifier_names, classifiers):\n",
        "  print(name)\n",
        "  clf.fit(x_rs, y_rs)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMRlZ6ZntrCp",
        "outputId": "9ed1c493-c935-4db0-9adf-e147ebc6ac09"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (525) in class 2 will be larger than the number of samples in the majority class (class #1 -> 225)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.91      0.52      0.66       175\n",
            "           2       0.44      0.88      0.59        75\n",
            "\n",
            "    accuracy                           0.63       250\n",
            "   macro avg       0.68      0.70      0.62       250\n",
            "weighted avg       0.77      0.63      0.64       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[91 84]\n",
            " [ 9 66]]\n",
            "\n",
            "Total Cost: 129\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.89      0.42      0.57       175\n",
            "           2       0.40      0.88      0.55        75\n",
            "\n",
            "    accuracy                           0.56       250\n",
            "   macro avg       0.64      0.65      0.56       250\n",
            "weighted avg       0.74      0.56      0.57       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[ 74 101]\n",
            " [  9  66]]\n",
            "\n",
            "Total Cost: 146\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.85      0.63      0.73       175\n",
            "           2       0.46      0.73      0.57        75\n",
            "\n",
            "    accuracy                           0.66       250\n",
            "   macro avg       0.65      0.68      0.65       250\n",
            "weighted avg       0.73      0.66      0.68       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[111  64]\n",
            " [ 20  55]]\n",
            "\n",
            "Total Cost: 164\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Cost-based Evaluation using Sample Weights***\n",
        "Now we want to use sample weights to evaluate the cost of the classification."
      ],
      "metadata": {
        "id": "lM1JGxSbZ9FI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we assign weights to each training instance. The weights can be defined using the default cost matrix or creating a custom cost matrix based on the credit amount."
      ],
      "metadata": {
        "id": "RFnwfx4pc9FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = np.zeros(y_train.shape[0])\n",
        "weights[np.where(y_train == 1)] = 1;\n",
        "weights[np.where(y_train == 2)] = 5;"
      ],
      "metadata": {
        "id": "FiNS7f8ZeCUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, clf in zip(classifier_names, classifiers):\n",
        "  print(name)\n",
        "  clf.fit(x_train, y_train, weights)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ],
      "metadata": {
        "id": "-l8Zb6dMaCOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e46e95a-71f8-4006-da75-254005aae4f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.87      0.70      0.77       175\n",
            "           2       0.51      0.75      0.61        75\n",
            "\n",
            "    accuracy                           0.71       250\n",
            "   macro avg       0.69      0.72      0.69       250\n",
            "weighted avg       0.76      0.71      0.72       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[122  53]\n",
            " [ 19  56]]\n",
            "\n",
            "Total Cost: 148\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.88      0.41      0.55       175\n",
            "           2       0.38      0.87      0.53        75\n",
            "\n",
            "    accuracy                           0.54       250\n",
            "   macro avg       0.63      0.64      0.54       250\n",
            "weighted avg       0.73      0.54      0.55       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[ 71 104]\n",
            " [ 10  65]]\n",
            "\n",
            "Total Cost: 154\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.86      0.68      0.76       175\n",
            "           2       0.50      0.75      0.60        75\n",
            "\n",
            "    accuracy                           0.70       250\n",
            "   macro avg       0.68      0.71      0.68       250\n",
            "weighted avg       0.75      0.70      0.71       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[119  56]\n",
            " [ 19  56]]\n",
            "\n",
            "Total Cost: 151\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above results we can conclude the following:\n",
        "- The Support Vector Machine (SVM) demonstrates a favorable outcome with minimal False Positives (FP); however, it concurrently exhibits the highest count of False Negatives (FN). This performance is less than ideal because while the model makes fewer errors in categorizing \"Bad\" customers as \"Good,\" it overlooks a significant number of customers by misclassifying them as \"Bad.\"\n",
        "- The Random Forest (RF) algorithm boasts the lowest cost and proves to be the more favorable choice. Although it slightly underperforms compared to the SVM in terms of False Positives (FP), with a difference of nine, it manages to retain a considerable number of \"Good\" customers, making it a more reliable option.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wVEQFhrSnSu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we evaluate the performance of the classifiers using the credit amount as weight. We keep the weight for class 1 (\"Good\") the same as before to be consistent with the documentation."
      ],
      "metadata": {
        "id": "SYzNsKe5eV1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = np.zeros(y_train.shape[0])\n",
        "weights[np.where(y_train == 1)] = 1\n",
        "weights[np.where(y_train == 2)] = x_train.loc[y_train == 2, 'Credit amount']"
      ],
      "metadata": {
        "id": "HeB_4PIbjAWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, clf in zip(classifier_names, classifiers):\n",
        "  print(name)\n",
        "  clf.fit(x_train, y_train, weights)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: \\n{conf_m}\\n')\n",
        "  print(f'Total Cost: {np.sum(conf_m * ORIGINAL_COST)}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ErHcG_Yn2Y2",
        "outputId": "f3a5c04f-74f7-4d19-ada9-cbd2e80a011a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forests\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.91      0.17      0.29       175\n",
            "           2       0.33      0.96      0.49        75\n",
            "\n",
            "    accuracy                           0.41       250\n",
            "   macro avg       0.62      0.57      0.39       250\n",
            "weighted avg       0.74      0.41      0.35       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[ 30 145]\n",
            " [  3  72]]\n",
            "\n",
            "Total Cost: 160\n",
            "\n",
            "Linear SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.85      0.19      0.31       175\n",
            "           2       0.33      0.92      0.48        75\n",
            "\n",
            "    accuracy                           0.41       250\n",
            "   macro avg       0.59      0.55      0.40       250\n",
            "weighted avg       0.69      0.41      0.36       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[ 33 142]\n",
            " [  6  69]]\n",
            "\n",
            "Total Cost: 172\n",
            "\n",
            "Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.88      0.41      0.55       175\n",
            "           2       0.38      0.87      0.53        75\n",
            "\n",
            "    accuracy                           0.54       250\n",
            "   macro avg       0.63      0.64      0.54       250\n",
            "weighted avg       0.73      0.54      0.55       250\n",
            "\n",
            "Confusion Matrix: \n",
            "[[ 71 104]\n",
            " [ 10  65]]\n",
            "\n",
            "Total Cost: 154\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above results we can conclude the following:\n",
        "- The Naive Bayes (NB) algorithm demonstrates the lowest cost among the models evaluated. While it exhibits a slightly higher count of False Positives (FP) compared to the Random Forest (RF) by seven and the Support Vector Machine (SVM) by four, it compensates with the lowest count of False Negatives (FN). This characteristic makes NB the preferred choice over the other algorithms.\n",
        "- The low count of False Positives (FP) in the Random Forest and SVM algorithms aligns with expectations, considering the potentially significant weight associated with misclassifying the second class."
      ],
      "metadata": {
        "id": "RIxB5rGmqWPV"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "flVdFqudJiDE",
        "ZXQg5ociOCZF",
        "1GVtmjEslhKE",
        "nEDy3MBmhzrj",
        "sZnQhx41ZcWz",
        "FHKaF4wZsEQr",
        "Vq9VwFVInqbk",
        "lM1JGxSbZ9FI"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}